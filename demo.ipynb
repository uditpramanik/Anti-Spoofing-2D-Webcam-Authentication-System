{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mtcnn\n",
    "from architecture import *\n",
    "from train_v2 import normalize, l2_normalizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from facetools import FaceDetection\n",
    "from facetools import IdentityVerification\n",
    "from facetools import LivenessDetection\n",
    "from facetools.utils import visualize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m root \u001b[38;5;241m=\u001b[39m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mabsolute()\n\u001b[0;32m      2\u001b[0m data_folder \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "root = Path(os.path.abspath(__file__)).parent.absolute()\n",
    "data_folder = root / \"data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resNet_checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[43mdata_folder\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInceptionResnetV1_vggface2.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m facebank_path \u001b[38;5;241m=\u001b[39m data_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreynolds.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m deepPix_checkpoint_path \u001b[38;5;241m=\u001b[39m data_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOULU_Protocol_2_model_0_0.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_folder' is not defined"
     ]
    }
   ],
   "source": [
    "resNet_checkpoint_path = data_folder / \"checkpoints\" / \"InceptionResnetV1_vggface2.onnx\"\n",
    "facebank_path = data_folder / \"reynolds.csv\"\n",
    "\n",
    "deepPix_checkpoint_path = data_folder / \"checkpoints\" / \"OULU_Protocol_2_model_0_0.onnx\"\n",
    "\n",
    "faceDetector = FaceDetection(max_num_faces=1)\n",
    "identityChecker = IdentityVerification(\n",
    "    checkpoint_path=resNet_checkpoint_path.as_posix(),\n",
    "    facebank_path=facebank_path.as_posix(),\n",
    ")\n",
    "livenessDetector = LivenessDetection(checkpoint_path=deepPix_checkpoint_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize face detection and recognition configurations\n",
    "confidence_t = 0.99\n",
    "recognition_t = 0.5\n",
    "required_size = (160, 160)\n",
    "\n",
    "# Initialize MediaPipe solutions\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Initialize MediaPipe Holistic Model for hand tracking\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.7)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands= mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "recognized_name = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and encode faces\n",
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)\n",
    "\n",
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        encoding_dict = pickle.load(f)\n",
    "    return encoding_dict\n",
    "\n",
    "def detect(img ,detector,encoder,encoding_dict):\n",
    "    global recognized_name  # Declare the global variable\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = 1 - dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "        else:\n",
    "            recognized_name = name  # Update the recognized name\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 255, 0), 2)\n",
    "            cv2.putText(img, name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (0, 200, 200), 2)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get fingertip position from hand landmarks\n",
    "def get_fingertip_position(hand_landmarks):\n",
    "    # Assuming index fingertip is used for pointing\n",
    "    fingertip = hand_landmarks.landmark[mp_holistic.HandLandmark.INDEX_FINGER_TIP]\n",
    "    return int(fingertip.x * frame_width), int(fingertip.y * frame_height)\n",
    "\n",
    "\n",
    "# Function to calculate distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and encode faces\n",
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)\n",
    "\n",
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        encoding_dict = pickle.load(f)\n",
    "    return encoding_dict\n",
    "\n",
    "def detect(img ,detector,encoder,encoding_dict):\n",
    "    global recognized_name  # Declare the global variable\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = 1 - dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "        else:\n",
    "            recognized_name = name  # Update the recognized name\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 255, 0), 2)\n",
    "            cv2.putText(img, name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (0, 200, 200), 2)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Function to get fingertip position from hand landmarks\n",
    "def get_fingertip_position(hand_landmarks):\n",
    "    # Assuming index fingertip is used for pointing\n",
    "    fingertip = hand_landmarks.landmark[mp_holistic.HandLandmark.INDEX_FINGER_TIP]\n",
    "    return int(fingertip.x * frame_width), int(fingertip.y * frame_height)\n",
    "\n",
    "\n",
    "# Function to calculate distance between two landmarks\n",
    "def calculate_distance(landmark1, landmark2):\n",
    "    return math.sqrt((landmark1.x - landmark2.x) ** 2 + (landmark1.y - landmark2.y) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if hand is showing an 'OK' sign\n",
    "def is_ok_sign(hand_landmarks):\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    index_pip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP]\n",
    "    \n",
    "    # Check if the thumb tip is near the index finger PIP joint (making a circle)\n",
    "    # and if the index finger tip is near the thumb tip.\n",
    "    thumb_index_distance = np.sqrt((thumb_tip.x - index_tip.x) ** 2 + \n",
    "                                   (thumb_tip.y - index_tip.y) ** 2 +\n",
    "                                   (thumb_tip.z - index_tip.z) ** 2)\n",
    "    index_pip_distance = np.sqrt((index_tip.x - index_pip.x) ** 2 + \n",
    "                                 (index_tip.y - index_pip.y) ** 2 +\n",
    "                                 (index_tip.z - index_pip.z) ** 2)\n",
    "    \n",
    "    if thumb_index_distance < 0.05 and thumb_index_distance < index_pip_distance:  # Threshold might need adjustment\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check if hand is showing a rock sign\n",
    "def is_rock_sign(hand_landmarks):\n",
    "    # Index and little fingers should be extended, middle and ring fingers not extended\n",
    "    return (hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y < \n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP].y and\n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y < \n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_MCP].y and\n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y > \n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_MCP].y and\n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].y > \n",
    "            hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP].y)\n",
    "\n",
    "# Function to check if hand is showing a thumbs-up sign\n",
    "def is_thumbs_up(hand_landmarks):\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    \n",
    "    # Thumb should be extended and above the MCP joint of the index finger\n",
    "    if thumb_tip.y < thumb_mcp.y and thumb_tip.y < index_mcp.y:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check if hand is showing a thumbs-down sign\n",
    "def is_thumbs_down(hand_landmarks):\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    thumb_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_MCP]\n",
    "    index_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_MCP]\n",
    "    \n",
    "    # Thumb should be extended and below the MCP joint of the index finger\n",
    "    if thumb_tip.y > thumb_mcp.y and thumb_tip.y > index_mcp.y:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to check if hand is showing a peace sign\n",
    "def is_peace_sign(hand_landmarks):\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "    ring_mcp = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_MCP]\n",
    "    \n",
    "    # Index and middle fingers should be extended, above the MCP of the ring finger\n",
    "    if index_tip.y < ring_mcp.y and middle_tip.y < ring_mcp.y:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly choose between a hand gesture and mouth state\n",
    "def choose_random_prompt():\n",
    "    prompts = ['Thumbs Up', 'Thumbs Down', 'Peace Sign', 'Rock Sign', 'OK Sign', 'Mouth Open', 'Mouth Closed']\n",
    "    return random.choice(prompts)\n",
    "\n",
    "displayed_prompt = choose_random_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'faceDetector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m liveness_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot Checked\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m canvas \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 28\u001b[0m faces, boxes \u001b[38;5;241m=\u001b[39m \u001b[43mfaceDetector\u001b[49m(frame)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_arr, box \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(faces, boxes):\n\u001b[0;32m     30\u001b[0m     min_sim_score, mean_sim_score \u001b[38;5;241m=\u001b[39m identityChecker(face_arr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'faceDetector' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    face_encoder = InceptionResNetV2()\n",
    "    path_m = \"facenet_keras_weights.h5\"\n",
    "    face_encoder.load_weights(path_m)\n",
    "    encodings_path = 'encodings/encodings.pkl'\n",
    "    face_detector = mtcnn.MTCNN()\n",
    "    encoding_dict = load_pickle(encodings_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"CAM NOT OPENED\")\n",
    "            break\n",
    "\n",
    "        display = 'No match'\n",
    "        liveness_result = 'Not Checked'\n",
    "\n",
    "        \n",
    "\n",
    "        canvas = frame.copy()\n",
    "        faces, boxes = faceDetector(frame)\n",
    "        for face_arr, box in zip(faces, boxes):\n",
    "            min_sim_score, mean_sim_score = identityChecker(face_arr)\n",
    "            liveness_score = livenessDetector(face_arr)\n",
    "            # Check if liveness score is greater than 0.03\n",
    "            liveness_result = 'Real' if liveness_score > 0.03 else 'Fake'\n",
    "            canvas = visualize_results(canvas, box, liveness_score, liveness_result)\n",
    "       \n",
    "\n",
    "        # Face detection and recognition\n",
    "        frame = detect(canvas, face_detector, face_encoder, encoding_dict)\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process with both Hands and Holistic models\n",
    "        hands_results = hands.process(frame_rgb)\n",
    "        holistic_results = holistic.process(frame_rgb)\n",
    "\n",
    "        # Convert back to BGR for rendering\n",
    "        frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Display the chosen prompt on screen\n",
    "        cv2.putText(frame_bgr, f'Match this: {displayed_prompt}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        match_found = False\n",
    "\n",
    "        # Hand gesture recognition\n",
    "        if hands_results.multi_hand_landmarks and not match_found:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                if is_thumbs_up(hand_landmarks):\n",
    "                    gesture = 'Thumbs Up'\n",
    "                elif is_thumbs_down(hand_landmarks):\n",
    "                    gesture = 'Thumbs Down'\n",
    "                elif is_peace_sign(hand_landmarks):\n",
    "                    gesture = 'Peace Sign'\n",
    "                elif is_rock_sign(hand_landmarks):\n",
    "                    gesture = 'Rock Sign'\n",
    "                elif is_ok_sign(hand_landmarks):\n",
    "                    gesture = 'OK Sign'\n",
    "                else:\n",
    "                    gesture = 'Unknown Gesture'\n",
    "\n",
    "                \n",
    "\n",
    "                # Check if user's gesture matches the displayed gesture\n",
    "                if gesture == displayed_prompt:\n",
    "                    cv2.putText(frame_bgr, 'Match!', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    display='Match!'\n",
    "                    match_found = True\n",
    "                    break\n",
    "\n",
    "        # Mouth state recognition\n",
    "        if holistic_results.face_landmarks and not match_found:\n",
    "            upper_lip = holistic_results.face_landmarks.landmark[13]  # Top of upper lip\n",
    "            lower_lip = holistic_results.face_landmarks.landmark[14]  # Bottom of lower lip\n",
    "            mouth_distance = calculate_distance(upper_lip, lower_lip)\n",
    "            \n",
    "            mouth_state = 'Mouth Closed'\n",
    "            if mouth_distance > 0.03:  # Threshold for open mouth\n",
    "                mouth_state = 'Mouth Open'\n",
    "\n",
    "            if mouth_state == displayed_prompt:\n",
    "                cv2.putText(frame_bgr, 'Match!', (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                display='Match!'\n",
    "            else:\n",
    "                cv2.putText(frame_bgr, 'No match', (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Check if 15 seconds have elapsed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time >= 30:\n",
    "            print(\"TIMEOUT!!!!. Retry....\")\n",
    "\n",
    "            # Load and display success image\n",
    "            error_image_path = 'timeout.jpg'  # Path to your success image\n",
    "            error_image = cv2.imread(error_image_path)\n",
    "            if error_image is not None:\n",
    "                cv2.imshow('Error', error_image)\n",
    "                cv2.waitKey(5000)  # Display the image for 5000 milliseconds (5 seconds)\n",
    "                cv2.destroyWindow('Error')\n",
    "            break\n",
    "\n",
    "\n",
    "        # Check conditions and display/print \"Welcome\" message\n",
    "        if recognized_name != 'unknown' and display == 'Match!' and liveness_result == 'Real':\n",
    "            welcome_message = \"Welcome\"\n",
    "            print(welcome_message)\n",
    "            \n",
    "\n",
    "            # Load and display success image\n",
    "            success_image_path = 'image.jpg'  # Path to your success image\n",
    "            success_image = cv2.imread(success_image_path)\n",
    "            if success_image is not None:\n",
    "                cv2.imshow('Success', success_image)\n",
    "                cv2.waitKey(7000)  # Display the image for 5000 milliseconds (5 seconds)\n",
    "                cv2.destroyWindow('Success')\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('camera', frame_bgr)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Press 'n' to display a new random prompt\n",
    "        if cv2.waitKey(10) & 0xFF == ord('n'):\n",
    "            displayed_prompt = choose_random_prompt()\n",
    "            match_found = False\n",
    "        \n",
    "        # Exit the loop on pressing 'q'\n",
    "        elif cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
